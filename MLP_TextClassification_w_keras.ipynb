{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import models\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_char = [chr(c + ord('0')) for c in range(10)]\n",
    "special_char.extend([' ', '~', '!', '@', '#', '$', '%', '^', '&', '-', '+', '=', \n",
    "                     '{', '}', '[', ']', '\\\\', '|', '/', '<', '>', '?', '“', '”', '\"',\n",
    "                    '‘', '’'])\n",
    "\n",
    "def is_valid_word(word):\n",
    "    return all(c not in word for c in special_char)\n",
    "\n",
    "def word_tokenize(sentence):\n",
    "    words, postags = ViPosTagger.postagging(ViTokenizer.tokenize(sentence.lower()))\n",
    "    return [word for word in words if is_valid_word(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['xahoi' , 'kinhdoanh', 'thethao', 'vanhoa']\n",
    "topic_names = ['Xã hội', 'Kinh doanh', 'Thể thao', 'Văn hóa']\n",
    "\n",
    "num_classes = len(topics)\n",
    "documents = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(topics)):\n",
    "    fn = os.path.join('data/headlines', topics[i] + '.txt')\n",
    "    f = open(fn, encoding='utf8')\n",
    "    documents.extend(f.readlines()[:5000])\n",
    "    labels.extend([i]*5000)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = list(map(word_tokenize, documents))\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_above=0.5, keep_n=10000)\n",
    "dict_size = len(dictionary)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(len(processed_docs)):\n",
    "    bow_vector = tfidf[bow_corpus[i]]\n",
    "    wordvec = np.zeros(dict_size)    \n",
    "    for index, value in bow_vector:\n",
    "        wordvec[index] = value\n",
    "    data.append((wordvec, labels[i]))\n",
    "    \n",
    "shuffle(data)\n",
    "Ntrain = int(len(data) * 0.7)\n",
    "\n",
    "X_train = np.array([item[0] for item in data[:Ntrain]])\n",
    "Y_train = np.array([to_categorical(item[1], num_classes) for item in data[:Ntrain]])\n",
    "\n",
    "X_test = np.array([item[0] for item in data[Ntrain:]])\n",
    "Y_test = np.array([to_categorical(item[1], num_classes) for item in data[Ntrain:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=dict_size, activation='sigmoid'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "14000/14000 [==============================] - 2s 125us/step - loss: 1.3388 - acc: 0.5232\n",
      "Epoch 2/40\n",
      "12128/14000 [========================>.....] - ETA: 0s - loss: 1.1890 - acc: 0.7998"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=40, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , score = model.evaluate(X_test, Y_test)\n",
    "print('score = ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Mới đây, Viện khảo cổ học phối hợp với Trung tâm Bảo tồn Di sản văn hoá thế giới Thành nhà Hồ tổ chức công bố kết quả khai quật thám sát di chỉ khảo cổ học núi Xuân Đài (xã Vĩnh Ninh, huyện Vĩnh Lộc, tỉnh Thanh Hóa).\"\"\"\n",
    "\n",
    "processed_text = word_tokenize(text)\n",
    "bow = dictionary.doc2bow(processed_text)\n",
    "bow_vector = tfidf[bow]\n",
    "\n",
    "wordvec = np.zeros(dict_size)\n",
    "for index, value in bow_vector:\n",
    "    wordvec[index] = value\n",
    "    \n",
    "predict = model.predict(np.array([wordvec]))\n",
    "categ = np.argmax(predict[0])\n",
    "print('Chủ đề : ', topic_names[categ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
