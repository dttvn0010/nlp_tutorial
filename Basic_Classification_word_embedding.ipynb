{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from random import shuffle\n",
    "from pyvi import ViTokenizer, ViPosTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_char = [chr(c + ord('0')) for c in range(10)]\n",
    "special_char.extend([' ', '~', '!', '@', '#', '$', '%', '^', '&', '-', '+', '=', \n",
    "                     '{', '}', '[', ']', '\\\\', '|', '/', '<', '>', '?', '“', '”', '\"',\n",
    "                    '‘', '’'])\n",
    "\n",
    "def is_valid_word(word):\n",
    "    return all(c not in word for c in special_char)\n",
    "\n",
    "def word_tokenize(sentence):\n",
    "    words, postags = ViPosTagger.postagging(ViTokenizer.tokenize(sentence.lower()))\n",
    "    return [word for word in words if is_valid_word(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['xahoi' , 'kinhdoanh', 'thethao', 'vanhoa']\n",
    "topic_names = ['Xã hội', 'Kinh doanh', 'Thể thao', 'Văn hóa']\n",
    "\n",
    "num_classes = len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_doc_counts = {}\n",
    "data = []\n",
    "\n",
    "for i in range(len(topics)):\n",
    "    fn = os.path.join('data/headlines', topics[i] + '.txt')\n",
    "    f = open(fn, encoding='utf8')\n",
    "    lines = f.readlines()[:5000]\n",
    "    \n",
    "    for line in lines:\n",
    "        tokens = word_tokenize(line.strip())\n",
    "        data.append((tokens, i))\n",
    "        \n",
    "        for token in set(tokens):\n",
    "            word_doc_counts[token] = word_doc_counts.get(token, 0) + 1\n",
    "        \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_items = list(word_doc_counts.items())\n",
    "word_items = sorted(word_items, key=lambda x : x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {item[0]:i+3 for i,item in enumerate(word_items)}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokens):    \n",
    "    return [word_index.get(token, 0) for token in tokens]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(data)\n",
    "Ntrain = int(0.7*len(data))\n",
    "train_data = data[:Ntrain]\n",
    "test_data = data[Ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = [encode_tokens(x[0]) for x in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = [encode_tokens(x[0]) for x in train_data]\n",
    "Xtrain = pad_sequences(Xtrain, value=0, padding='post', maxlen=128)\n",
    "ytrain = np.array([to_categorical(x[1], num_classes) for x in train_data])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = [encode_tokens(x[0]) for x in test_data]\n",
    "Xtest = pad_sequences(Xtest, value=0, padding='post', maxlen=128)\n",
    "ytest = np.array([to_categorical(x[1], num_classes) for x in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(Xtrain, ytrain, epochs=60, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Mới đây, Viện khảo cổ học phối hợp với Trung tâm Bảo tồn Di sản văn hoá thế giới Thành nhà Hồ tổ chức công bố kết quả khai quật thám sát di chỉ khảo cổ học núi Xuân Đài (xã Vĩnh Ninh, huyện Vĩnh Lộc, tỉnh Thanh Hóa).\"\"\"\n",
    "tokens = word_tokenize(text)\n",
    "words_id = encode_tokens(tokens)\n",
    "X = pad_sequences([words_id], value=0, padding='post', maxlen=128)\n",
    "y = np.argmax(model.predict(X))\n",
    "print(topic_names[y])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
